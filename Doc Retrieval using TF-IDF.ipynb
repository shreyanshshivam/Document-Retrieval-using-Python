{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Document Retrieval \n",
    "\n",
    "### Objectives \n",
    "-   Concepts: Sparse vectorss  \n",
    "-   Represent documents into vectors using Gensim. - TFIDF Models\n",
    "-   Application Document Similarity - To use Gensim to compute similarities between documents.\n",
    "-   Application TDocument Retrieval - To use Gensim to rank documents based on relevance to a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Vectors\n",
    "\n",
    "\n",
    "To convert a piece of text into a vector, we need to first determine the\n",
    "dimension of the vector, or in other words the number of components of\n",
    "the vector. Generally, the dimension of the vectors representing\n",
    "documents (and thus the dimension of the vector space) is the same as\n",
    "the vocabulary size, that is, each unique word corresponds to an entry\n",
    "of the vectors. Here vocabulary refers to the set of all unique words in\n",
    "a corpus.\n",
    "\n",
    "While finding out the vocabulary size is not a problem — you may want to\n",
    "think about how to do this — the real problem is that for a real world\n",
    "corpus, the vocabulary is usually very large. It is not uncommon to have\n",
    "millions of words in a vocabulary. If we represent each document as a\n",
    "very high-dimensional vector, we will need a lot of space to store these\n",
    "vectors either in memory or on disk. In reality, however, each document\n",
    "contains only a relatively small set of words, so the vector used to\n",
    "represent a document has most entries equal to zero and a small subset\n",
    "of entries with non-zero values. When we store such kind of vectors in a\n",
    "computer, we typically use a *sparse vector* representation. In this\n",
    "representation, we only need to specify the values of those non-zero\n",
    "entries of a vector.\n",
    "\n",
    "For example, suppose our vocabulary has 10 words. Let us look at the\n",
    "following vector: $$\\begin{aligned}\n",
    "v & = &\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "2 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0.5 \\\\\n",
    "\\end{pmatrix}.\\end{aligned}$$ To store this vector in its original form,\n",
    "we need to store 10 numbers, each corresponding to one entry of the\n",
    "vector. A sparse vector representation stores only the non-zero entries\n",
    "as follows: $$\\begin{aligned}\n",
    "v & = & \\left( (0, 1), (3, 2), (9, 0.5) \\right).\\end{aligned}$$ Here the\n",
    "sparse vector is a list of pairs. For each pair, the first number is an\n",
    "ID or index indicating a particular entry of the original vector. For\n",
    "example, $(0, 1)$ indicates that the first entry of the original $v$ has\n",
    "a value of 1, and $(9, 0.5)$ indicates that the tenth entry of the\n",
    "original $v$ has a value of 0.5. We can see that the amount of space\n",
    "needed to store this sparse vector is now reduced to twice the number of\n",
    "non-zero entries of the original vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the sparse vectors for the SGNews dataset\n",
    "\n",
    "-  Create the corpus, sg_corpus using gensim \n",
    "\n",
    "-  Create a dictionary for the SGNews_Apr2012 corpus\n",
    "\n",
    "-  Create the processed sg_corpus and store in sg_stem. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "# Creating a dictionary from SGNews Apr2012.\n",
    "#import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sg_corpus = PlaintextCorpusReader('data/SGNews_Apr2012', '.+\\.txt')\n",
    "sg_fids = sg_corpus.fileids()\n",
    "\n",
    "sg_docs=[]\n",
    "for file in sg_fids:\n",
    "    with open('data/SGNews_Apr2012/'+file, 'r') as file_to_read:\n",
    "        doc = file_to_read.read()\n",
    "        sg_docs.append(doc)\n",
    "sg_docs=[list(gensim.utils.tokenize(f)) for f in sg_docs]\n",
    "sg_lower = [[w.lower() for w in doc] for doc in sg_docs]\n",
    "sg_stop = [[w for w in doc if w not in gensim.parsing.preprocessing.STOPWORDS] for doc in sg_lower]\n",
    "sg_stem = [[stemmer.stem(w) for w in doc] for doc in sg_stop]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Representation as dense vectors\n",
    "\n",
    "### Creating a Dictionary from a Corpus \n",
    "\n",
    "Now think about converting all documents in a corpus into vectors. We\n",
    "need to map each unique word in the vocabulary of this corpus to an ID\n",
    "or index first. These mappings from words to IDs are represented by a\n",
    "class called `Dictionary` in Gensim.\n",
    "\n",
    "In order to work on text documents, Gensim requires the words (aka tokens) be converted to unique ids. In order to achieve that, Gensim lets you create a Dictionary object that maps each word to a unique id.\n",
    "\n",
    "\n",
    "Assume that `sg_stem` represents our corpus after several steps of\n",
    "pre-processing. Note that `sg_stem` is a list of lists. The code below shows how a dictionary is created from\n",
    "`sg_stem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary\n",
    "sg_dictionary = corpora.Dictionary(sg_stem)\n",
    "\n",
    "print(sg_dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import Gensim first and then import the `corpora` module from\n",
    "Gensim. The `print` function is used to display the generated\n",
    "dictionary. If you have correctedly performed the pre-processing steps such as stop word removal, you should see that there are <b> 6337 unique tokens (words) </b> in the\n",
    "corpus, and only a few of them are shown.\n",
    "\n",
    "What if we want to see all the mappings from tokens to IDs in this\n",
    "dictionary? We can use `dictionary.token2id` to obtain a `dict` object which contains all the\n",
    "mappings. See below. `dict` is built-in data type (and\n",
    "class) in Python. It can be used to store mappings from some elements to\n",
    "some other elements. For those of you who have programming background,\n",
    "this is similar to a hash table (e.g., the HashMap class in Java). The\n",
    "code below shows how we use a variable\n",
    "`token_to_id` to store the mappings from tokens to IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = sg_dictionary.token2id\n",
    "print(type(token_to_id))\n",
    "print(token_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, \"letter\" corresponds to ID 71, indicating that the $71^{\\text{th}}$\n",
    "entry of a vector in this vector space represents \"letter\". (Note that since we have performed stemming, not\n",
    "all tokens are meaningful English words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sg_dictionary.token2id['banner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the function doc2bow to convert doc to another list which we call vec (to indicate that this is a vector). (Here bow stands for bag of words, meaning that the order of the words in the original document is ignored and we treat a document as a bag of words without any order.)\n",
    "As we can see below, the generated vec for the second file is a list where each element is a pair of numbers. For example, the first element of vec is (3, 1), and the second element of vec is (8, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all documents in SGNews Apr2012 to a list of sparse vectors. Store in sg_vecs variable\n",
    "sg_vecs = [sg_dictionary.doc2bow(doc) for doc in sg_stem]\n",
    "print(sg_vecs[1][0:10]) #prints the 10 words in the second file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application - Document Similarity\n",
    "#### Computing Similarities between Documents\n",
    "\n",
    "\n",
    "Gensim has built-in functions to compute cosine similarities between\n",
    "documents. Let us use a simple example to see how it works. We first\n",
    "define a very small corpus with only two documents, represented as\n",
    "sparse vectors below. We can see that the vector\n",
    "space of this corpus has a dimension of 4 (because we see only the IDs\n",
    "0, 1, 2 and 3 in the vectors).\n",
    "\n",
    "Given this corpus, we first build an index for more efficient\n",
    "computation later on, as shown below. Note that to\n",
    "build an index from a list of sparse vectors, we need to also give the\n",
    "dimension of the vector space, thus the argument `4` in\n",
    "`similarities.SparseMatrixSimilarity(toy_corpus, 4)`.\n",
    "\n",
    "You do not need to understand full details of how this index looks like and\n",
    "exactly why we need such an index. If you are interested, you can read\n",
    "up on *inverted index* (<http://en.wikipedia.org/wiki/Inverted_index>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "toy_corpus = [[(0,1),(1,1),(2,1)],[(1,2),(3,1)]]\n",
    "index = similarities.SparseMatrixSimilarity(toy_corpus,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have a new document represented as `[(0, 1)]`. That is,\n",
    "this new document has only a single word corresponding to ID 0. To\n",
    "compute the similarities between this document and all documents in\n",
    "`toy_corpus`, refer to the code below. Here `index[test_doc]`\n",
    "returns all the cosine similarity measures between the document\n",
    "`test_doc` and documents in `toy_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc = [(0,1)]\n",
    "sims = index[test_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display these similarity measures, we can use the code shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the output shows is that the cosine similarity\n",
    "between `test_doc` and the first document in the index is around 0.577,\n",
    "while the similarity between `test_doc` and the second document is 0.0.\n",
    "\n",
    "We can try another test document, as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = [(1,1), (3,1)]\n",
    "sims = index[test_doc]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent documents as TFIDF Vectors  - TF-IDF Weighting (Optional)\n",
    "\n",
    "We can use Gensim to help compute inverse document frequencies (IDFs)\n",
    "and use them to re-assign weights to document vectors. To do this, we\n",
    "need to import `models` from Gensim first. See below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(toy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code above simply computes the inverse document\n",
    "frequencies for all words in the input corpus. The formula it uses to\n",
    "compute IDF is the following: $$\\begin{aligned}\n",
    "\\textit{idf}_w  & = & \\log_2 \\frac{D}{\\textit{df}_w},\\end{aligned}$$\n",
    "where $D$ is the total number of documents in the corpus and\n",
    "$\\textit{df}_w$ is the document frequency of word $w$, i.e., the number\n",
    "of documents in which $w$ occurs.\n",
    "\n",
    "Recall that our toy corpus has 4 words. Using\n",
    "the formula above, we can manually calculate the IDF values of the four\n",
    "words. For example, for the word with ID 0, its document frequency is 1,\n",
    "and the corpus has 2 documents, so its inverse document frequency is\n",
    "$\\log_2 2 = 1$. For the word with ID 1, its document frequency is 2 and\n",
    "the corpus also has 2 documents, so its inverse document frequency is\n",
    "$\\log_2 1 = 0$.\n",
    "\n",
    "Now let us define a test document as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc = [(0,1), (1,1), (2,1), (3,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This document contains all the words in the vocabulary of the toy\n",
    "corpus, and every word occurs only once. We can use the `tfidf` object\n",
    "to transform `test_doc`, as shown in below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf[test_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we can see that in the transformed vector, we no longer\n",
    "see an entry corresponding to the word with ID 1. This makes sense\n",
    "because the IDF of this word is 0, and therefore after TF-IDF weighting,\n",
    "this word has a value of 0 in the vector and does not need to appear in\n",
    "the sparse vector representation. However, for all the other words,\n",
    "their IDF values are 1. Shouldn’t the converted vector be\n",
    "`[(0, 1), (2, 1), (3, 1)]`?\n",
    "\n",
    "The reason we do not see `[(0, 1), (2, 1), (3, 1)]` is that the `tfidf`\n",
    "object automatically *normalizes* the vectors when it transforms them.\n",
    "When a vector is normalized, the value of each entry is divided by the\n",
    "*norm* of this vector, such that the norm of the new vector is exactly\n",
    "1.\n",
    "\n",
    "So for the sparse vector `[(0, 1), (2, 1), (3, 1)]`, mathematically it\n",
    "is represented as $$\\begin{aligned}\n",
    "v & = &\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{pmatrix}.\\end{aligned}$$ We can see that when we normalize $v$, it\n",
    "becomes $$\\begin{aligned}\n",
    "v' & = & \\frac{v}{\\|v\\|} \\\\\n",
    "& = & \\frac{v}{\\sqrt{3}} \\\\\n",
    "& = &\n",
    "\\begin{pmatrix}\n",
    "0.5774 \\\\\n",
    "0 \\\\\n",
    "0.5774 \\\\\n",
    "0.5774 \\\\\n",
    "\\end{pmatrix}.\\end{aligned}$$\n",
    "\n",
    "Normalization does not affect cosine similarities between two vectors.\n",
    "In fact, if all vectors have been normalized, then to compute their\n",
    "cosine similarities, we only need to compute their *dot products*\n",
    "because every vector’s norm is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SG_News Corpus to TFIDF Vectors \n",
    "\n",
    "1. Based on the example above, create a `TfidfModel` object from\n",
    "the `SGNews_Apr2012` corpus\n",
    "2. Transform all the document\n",
    "vectors in this corpus with TF-IDF weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the sg_vecs to tfidf vectors using TfidfModel function\n",
    "\n",
    "sg_tfidf = models.TfidfModel(sg_vecs)\n",
    "sg_vecs_with_tfidf = [sg_tfidf[vec] for vec in sg_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sg_vecs_with_tfidf[1][0:10]) #prints the 10 words in the second file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application - Document Retrieval\n",
    "#### Matching and Ranking the documents \n",
    "\n",
    "The last part (and maybe the most exciting part as well) of this lab is\n",
    "to see how cosine similarity with TF-IDF weighting can be used for the\n",
    "task of document retrieval. Given a query represented as a set of words,\n",
    "the goal of document retrieval is to find a list of documents from a\n",
    "corpus that are the most relevant to the query. This can be done by\n",
    "ranking all the documents in the corpus based on their cosine\n",
    "similarities to the query.\n",
    "\n",
    "Recall the following variables have been defined:\n",
    "\n",
    "-   `sg_dictionary`: The dictionary that contains the mappings from\n",
    "    unique words in the `SGNews_Apr2012` corpus to integer IDs.\n",
    "\n",
    "-   `sg_tfidf`: An `TfidfModel` object that stores the IDF values\n",
    "    derived from this corpus and can be used to transform document\n",
    "    vectors that have not been weighted with IDF.\n",
    "-   `sg_vecs_with_tfidf`: A list of sparse vectors representing all the\n",
    "    documents in this corpus. TF-IDF weighting has been performed.\n",
    "\n",
    "\n",
    "Below are the steps:\n",
    "1. Create index\n",
    "2. Create the query vector after processing\n",
    "3. Convert query tfidf vector\n",
    "4. Apply the index on this query and get the sorted similarity score list of documents \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sg_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Why we need this? Why did we input 6337?\n",
    "sg_index = similarities.SparseMatrixSimilarity(sg_vecs_with_tfidf, 6337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Next, we define a query called `query1` that consists of two words:\n",
    "“Malaysia” and “Singapore.” We need to use lowercase letters and perform\n",
    "stemming such that the query is processed in the same way as the\n",
    "documents in the corpus. We then convert the query into a sparse vector\n",
    "using `doc2bow()`. After that, we perform TF-IDF weighting on the\n",
    "vector. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = [stemmer.stem('malaysia'), stemmer.stem('singapore')]\n",
    "query1_vec = sg_dictionary.doc2bow(query1)\n",
    "print(query1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1_vec_tfidf = sg_tfidf[query1_vec]\n",
    "print(query1_vec_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To verify that the transformation is correct,\n",
    "the code below shows the IDs of the stemmed “malaysia” and\n",
    "“singapore.” We can see that `query1_vec` contains both words, whereas\n",
    "after TF-IDF weighting, `query1_vec_tfidf` contains only the stemmed “malaysia.” \n",
    "This is because\n",
    "the word “Singapore” occurs in all the documents in this corpus, and\n",
    "therefore its IDF value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sg_dictionary.token2id[stemmer.stem('malaysia')])\n",
    "print(sg_dictionary.token2id[stemmer.stem('singapore')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Next, we compute the cosine similarities between the query and all\n",
    "documents in the corpus. This is shown in below. You can\n",
    "probably guess that the function `sorted` is used to sort a bunch of\n",
    "elements. But what is `key=lambda item: -item[1]`? Do not worry too much\n",
    "about this. You will not likely need to modify this part so it is OK if\n",
    "you do not understand it. Essentially this code defines how the elements\n",
    "should be sorted. You can see that they are sorted by the similarity\n",
    "scores, which are always the second element in those pairs in round\n",
    "brackets (thus `item[1]`). Also, sorting is in descending order (thus\n",
    "the negative sign in front of `item[1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_sims = sg_index[query1_vec_tfidf]\n",
    "q1_sorted_sims = sorted(enumerate(q1_sims), key = lambda item: -item[1])\n",
    "print(q1_sorted_sims[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the code shows is that the Rank 1 relevant document is\n",
    "the document with an index of 206 in the corpus, the Rank 2 document is\n",
    "document with index 138, and so on. To verify that these are indeed\n",
    "documents related to the query, the code finds out the\n",
    "original file IDs of those documents. Recall that the file IDs can be\n",
    "retrieved using the function `fileids()`. By specifying the index into\n",
    "the list of file IDs, we can get the original file names. Now open the\n",
    "original text files with these IDs on your computer and check whether\n",
    "they are indeed related to Malaysia. Also check whether the ones ranked\n",
    "higher generally contain more occurrences of the word “Malaysia” and are\n",
    "generally shorter. (Think about why.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
